{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683157da-a083-4cc2-b16b-5824b5e2a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is anomaly detection and what is its purpose?\n",
    "\n",
    "# Q2. What are the key challenges in anomaly detection?\n",
    "\n",
    "# Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\n",
    "# Q4. What are the main categories of anomaly detection algorithms?\n",
    "\n",
    "# Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "\n",
    "# Q6. How does the LOF algorithm compute anomaly scores?\n",
    "\n",
    "# Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "\n",
    "# Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score \n",
    "# using KNN with K=10?\n",
    "\n",
    "# Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the \n",
    "# anomaly score for a data point that has an average path length of 5.0 compared to the average path \n",
    "# length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba2ddd9-2811-4c6f-8fee-d65fce84bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b7d55c-0085-458f-8983-3c96a097f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection refers to the process of identifying patterns or instances that deviate significantly from the expected behavior within a dataset. \n",
    "# The purpose of anomaly detection is to identify unusual or rare observations that do not conform to the typical patterns or behaviors of the system \n",
    "# or dataset under consideration. These anomalies can be caused by various factors such as errors, outliers, fraudulent activities, faults, or other abnormal events.\n",
    "\n",
    "# The primary goal of anomaly detection is to highlight or flag these unusual instances for further investigation, as they often represent important \n",
    "# and potentially valuable information. By identifying anomalies, analysts or systems can focus their attention on these specific instances and \n",
    "# take appropriate actions, such as further analysis, intervention, or mitigation, depending on the context.\n",
    "\n",
    "# Anomaly detection is widely used in various domains, including finance, cybersecurity, network monitoring, manufacturing, healthcare, and many others. \n",
    "# It helps in identifying fraud or malicious activities, detecting technical glitches or faults, predicting equipment failures, monitoring system performance, \n",
    "# and ensuring the overall security and integrity of a system or dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2542bef-e11f-491c-b07e-a41ecdcede87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31c0e610-ec78-4bde-9a6a-520cd843bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection comes with several challenges that need to be addressed for accurate and effective detection. Here are some key challenges in anomaly detection:\n",
    "\n",
    "# Lack of labeled data: Anomaly detection often requires labeled data, with anomalies correctly identified, for training supervised algorithms. \n",
    "# However, obtaining labeled data can be difficult, as anomalies are often rare and may require expert knowledge to annotate.\n",
    "\n",
    "# Imbalanced datasets: Anomalies are typically a minority class, leading to imbalanced datasets where normal instances significantly outnumber anomalous ones.\n",
    "# This class imbalance can affect the performance of anomaly detection algorithms, as they may be biased towards the majority class \n",
    "# and struggle to detect anomalies accurately.\n",
    "\n",
    "# Evolving anomalies: Anomalies can change over time as new patterns emerge or existing ones evolve. Anomaly detection systems should be able to adapt \n",
    "# and detect new types of anomalies without relying solely on historical data.\n",
    "\n",
    "# Variability and context: Anomalies can be subjective and context-dependent. What is considered an anomaly in one context may be normal in another.\n",
    "# Capturing the variability and context of anomalies is a challenge, as it requires understanding the specific characteristics and dynamics of the system\n",
    "# or dataset under consideration.\n",
    "\n",
    "# Feature selection and dimensionality: Choosing relevant features or variables that effectively capture the anomalies is crucial. \n",
    "# In high-dimensional datasets, selecting informative features and handling the curse of dimensionality can be challenging and \n",
    "# impact the performance of anomaly detection algorithms.\n",
    "\n",
    "# Noise and outliers: Distinguishing between anomalies and noise or outliers that do not represent meaningful anomalies can be difficult. \n",
    "# Noise in the data can introduce false positives or obscure genuine anomalies, requiring robust techniques to handle noise and outliers effectively.\n",
    "\n",
    "# Real-time detection: For applications where anomalies need to be detected in real-time, such as network monitoring or cybersecurity, processing data\n",
    "# and detecting anomalies within strict time constraints pose additional challenges in terms of computational efficiency and latency.\n",
    "\n",
    "# Addressing these challenges often requires a combination of advanced techniques such as machine learning, statistical modeling, domain expertise, \n",
    "# and continuous monitoring and adaptation of the anomaly detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b80510cc-9cd6-477b-9413-de8e84fe22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f47ddb-85fd-45da-9545-a83039e76a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised anomaly detection and supervised anomaly detection differ in their approach to detecting anomalies and the type of data they require.\n",
    "# Here's an overview of the differences:\n",
    "\n",
    "# Data requirements:\n",
    "\n",
    "# Unsupervised Anomaly Detection: Unsupervised methods operate on unlabeled data, where anomalies are not explicitly identified during the training phase.\n",
    "# These algorithms aim to identify patterns or instances that deviate significantly from the expected behavior within the dataset.\n",
    "# Supervised Anomaly Detection: Supervised methods require labeled data, where anomalies are pre-identified and labeled during the training phase.\n",
    "# The algorithm learns the patterns and characteristics of anomalies from the labeled data to detect similar anomalies in unseen data.\n",
    "\n",
    "# Training process:\n",
    "\n",
    "# Unsupervised Anomaly Detection: Unsupervised methods rely on the inherent structure or distribution of the data to identify anomalies. \n",
    "# They typically model the normal behavior of the data and flag instances that deviate from this learned representation as anomalies. \n",
    "# Unsupervised methods include techniques like clustering, density estimation, and distance-based methods.\n",
    "# Supervised Anomaly Detection: Supervised methods train a model on labeled data, where anomalies are explicitly identified. \n",
    "# The model learns to distinguish between normal and anomalous instances based on the labeled examples. \n",
    "# Common supervised techniques for anomaly detection include decision trees, support vector machines (SVM), and neural networks.\n",
    "\n",
    "# Performance evaluation:\n",
    "\n",
    "# Unsupervised Anomaly Detection: Evaluating the performance of unsupervised methods is challenging due to the lack of labeled anomalies during training.\n",
    "# Evaluation metrics focus on comparing the detected anomalies with expert knowledge or ground truth labels when available. \n",
    "# Common evaluation measures include precision, recall, F1-score, and area under the receiver operating characteristic (ROC) curve.\n",
    "# Supervised Anomaly Detection: Supervised methods can directly evaluate their performance using standard classification metrics,\n",
    "# as they have access to labeled anomalies during training. Metrics such as accuracy, precision, recall, F1-score, \n",
    "# and ROC curve analysis can be used to assess the model's performance.\n",
    "\n",
    "# Applicability:\n",
    "\n",
    "# Unsupervised Anomaly Detection: Unsupervised methods are useful when labeled anomaly data is scarce or difficult to obtain. \n",
    "# They can discover novel or unknown anomalies that may not have been explicitly labeled in the training data. \n",
    "# Unsupervised methods are often applied in situations where the anomaly patterns may evolve or change over time.\n",
    "# Supervised Anomaly Detection: Supervised methods are suitable when labeled anomaly data is available. \n",
    "# They work well in scenarios where the anomalies of interest are already identified and can be used to train a model to detect similar anomalies. \n",
    "# Supervised methods are effective when there is a clear distinction between normal and anomalous instances.\n",
    "\n",
    "# Both unsupervised and supervised methods have their advantages and limitations, and the choice depends on the availability of labeled data, the nature of anomalies,\n",
    "# and the specific requirements of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a76b723-1519-4b44-8881-072626cecbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0890831b-52df-41c4-add3-e96e3468baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Anomaly detection algorithms can be broadly categorized into the following main categories:\n",
    "\n",
    "# Statistical Methods: Statistical methods assume that the normal data follows a known statistical distribution, such as Gaussian (normal) distribution.\n",
    "# Anomalies are then detected as instances that significantly deviate from the expected statistical properties of the data. \n",
    "# Statistical methods include techniques like z-score, percentile rank, probability density estimation, and hypothesis testing.\n",
    "\n",
    "# Machine Learning Methods:\n",
    "# a. Unsupervised Learning: Unsupervised learning methods aim to identify anomalies based on the inherent structure or patterns within the data. \n",
    "# Clustering algorithms, such as k-means or DBSCAN, can be used to identify groups or clusters of similar instances, and anomalies are considered \n",
    "# as instances that do not belong to any cluster or are in small or sparse clusters.\n",
    "# b. Supervised Learning: Supervised learning methods require labeled data, where anomalies are explicitly identified\n",
    "# . These algorithms learn the patterns and characteristics of anomalies from the labeled examples and can detect similar anomalies in unseen data.\n",
    "# Supervised anomaly detection techniques include decision trees, support vector machines (SVM), neural networks, and ensemble methods.\n",
    "\n",
    "# Distance-Based Methods: Distance-based methods measure the similarity or dissimilarity between instances in the dataset. \n",
    "# Anomalies are identified as instances that have a significantly different distance or dissimilarity compared to the majority of instances.\n",
    "# Distance-based methods include k-nearest neighbors (k-NN), local outlier factor (LOF), and Mahalanobis distance.\n",
    "\n",
    "# Density-Based Methods: Density-based methods focus on estimating the density of the data and identifying anomalies as instances that have significantly\n",
    "# low density compared to the majority of instances. Techniques such as Gaussian mixture models (GMM), kernel density estimation (KDE), and one-class\n",
    "# SVM fall under this category.\n",
    "\n",
    "# Information-Theoretic Methods: Information-theoretic methods measure the amount of information or surprise provided by an instance in the dataset. \n",
    "# Anomalies are identified as instances that provide a high amount of information or have a low probability according to the underlying probability distribution. \n",
    "# Information-theoretic methods include techniques like mutual information, entropy-based methods, and novelty detection using autoencoders.\n",
    "\n",
    "# Ensemble Methods: Ensemble methods combine multiple anomaly detection algorithms or models to improve the overall detection performance.\n",
    "# They leverage the diversity of individual models to enhance anomaly detection accuracy and robustness.\n",
    "# Ensemble methods can be applied to various anomaly detection techniques, including statistical methods, machine learning methods, or distance-based methods.\n",
    "\n",
    "# It's important to note that these categories are not mutually exclusive, and some algorithms can belong to multiple categories.\n",
    "# The choice of algorithm depends on the characteristics of the data, the availability of labeled data, the complexity of anomalies,\n",
    "# and the specific requirements of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "969ffdad-2d04-4d8a-a3de-1302d74e9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8891c310-398f-47a7-a1bf-35eeb8c5b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance-based anomaly detection methods make certain assumptions about the data and the characteristics of anomalies.\n",
    "# Here are the main assumptions typically made by distance-based anomaly detection methods:\n",
    "\n",
    "# Distance metric: Distance-based methods assume that a meaningful distance or dissimilarity measure can be defined to quantify the similarity between instances i\n",
    "# n the dataset. Common distance metrics include Euclidean distance, Mahalanobis distance, or cosine similarity. The choice of distance metric depends\n",
    "# on the nature of the data and the specific requirements of the problem.\n",
    "\n",
    "# Normal data distribution: Distance-based methods often assume that the majority of the data follows a specific distribution or has a certain structure. \n",
    "# This assumption helps in defining a reference or expected behavior against which anomalies are identified. For example, in the case of k-nearest neighbors \n",
    "# (k-NN) method, it assumes that the majority of instances are densely clustered, and anomalies are located in sparsely populated regions.\n",
    "\n",
    "# Neighborhood-based anomaly assumption: Distance-based methods assume that anomalies are located in regions of the feature space that have a lower density\n",
    "# or a different distribution compared to normal instances. Anomalies are considered as instances that have a larger distance or dissimilarity to\n",
    "# their nearest neighbors or to the majority of instances in the dataset.\n",
    "\n",
    "# Independence assumption: Some distance-based methods, such as local outlier factor (LOF), assume that the attributes or features of the data are independent \n",
    "# of each other. This assumption allows the detection of anomalies based on the local density of instances without considering the global data distribution.\n",
    "\n",
    "# Homogeneity assumption: Distance-based methods assume that the majority of instances in the dataset are homogeneous and share similar characteristics.\n",
    "# Anomalies, on the other hand, are expected to deviate significantly from this homogeneity and exhibit distinct properties.\n",
    "\n",
    "# Known or fixed number of neighbors: Certain distance-based methods, like k-NN, assume a fixed number of neighbors to determine the local density or dissimilarity. \n",
    "# This assumption may limit the effectiveness of these methods in scenarios where the optimal number of neighbors varies or is unknown.\n",
    "\n",
    "# It's important to note that these assumptions may not always hold in all scenarios, and the performance of distance-based methods can be affected if\n",
    "# the assumptions are violated. Therefore, it's crucial to carefully consider the data and the specific characteristics of anomalies before applying\n",
    "# distance-based anomaly detection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "366542e3-9887-4b2f-b90c-19858006b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bafedec-4e75-45a1-a660-0f17612ed160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Local Outlier Factor (LOF) algorithm computes anomaly scores based on the local density of instances in the dataset. \n",
    "# The steps involved in calculating anomaly scores using the LOF algorithm are as follows:\n",
    "\n",
    "# Computing local reachability distance (LRD):\n",
    "\n",
    "# For each instance in the dataset, the distance to its k-nearest neighbors is calculated.\n",
    "# The local reachability distance (LRD) of an instance is then computed as the inverse of the average reachability distance of its k-nearest neighbors.\n",
    "# The reachability distance measures the distance between an instance and its neighbors.\n",
    "# Computing local outlier factor (LOF):\n",
    "\n",
    "# For each instance, the LOF is calculated by comparing its local density with the local densities of its neighbors.\n",
    "# The local density of an instance is determined by the average LRD of its k-nearest neighbors.\n",
    "# The LOF of an instance is computed as the ratio of the average local density of its k-nearest neighbors to its own local density.\n",
    "# Computing anomaly scores:\n",
    "\n",
    "# The anomaly score of an instance is obtained by averaging the LOF values of its k-nearest neighbors.\n",
    "# An instance with a high LOF value compared to its neighbors indicates that it has a lower density than its surrounding neighbors,\n",
    "# suggesting it is likely to be an anomaly. Consequently, it will have a higher anomaly score.\n",
    "# The LOF algorithm assigns anomaly scores to each instance in the dataset based on the relative local densities and distances to neighbors.\n",
    "# Higher LOF values indicate instances that are more likely to be anomalies, as they have lower local densities compared to their neighbors.\n",
    "# The LOF scores provide a measure of the degree of outlierness of each instance in the dataset.\n",
    "\n",
    "# By examining the anomaly scores, analysts can prioritize instances with higher scores for further investigation or intervention, \n",
    "# as they are more likely to be anomalous compared to instances with lower scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "409f3acf-b106-42a5-a53e-d04a980b18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. What are the key parameters of the Isolation Forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ce06caa-24b0-4f7e-bcb7-95f60e3adcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Isolation Forest algorithm, a popular anomaly detection algorithm, has several key parameters that can be adjusted to control its behavior.\n",
    "# The main parameters of the Isolation Forest algorithm are as follows:\n",
    "\n",
    "# Number of Trees (n_estimators): This parameter determines the number of isolation trees to be created. Increasing the number of trees generally improves\n",
    "# the performance of the algorithm but also increases the computational complexity.\n",
    "\n",
    "# Subsample Size (max_samples): It represents the size of the random subsets of the dataset used to build each isolation tree. \n",
    "# A smaller subsample size can increase the speed of the algorithm but may also reduce its effectiveness. The default value is set to \"auto,\"\n",
    "# which corresponds to using a subsample size of the minimum between 256 and the dataset size.\n",
    "\n",
    "# Maximum Tree Depth (max_depth): This parameter controls the maximum depth allowed for each isolation tree. A deeper tree can lead to overfitting,\n",
    "# while a shallow tree may not capture complex anomalies. The default value is set to \"None,\" which means the trees are grown until all instances are isolated \n",
    "# or a minimum number of instances is reached.\n",
    "\n",
    "# Contamination: It specifies the expected percentage of anomalies or outliers in the dataset. This parameter helps in defining the threshold \n",
    "# for classifying instances as anomalies. The default value is set to 0.1, corresponding to 10% of the dataset being considered as anomalies.\n",
    "\n",
    "# Random Seed (random_state): It determines the random number generator seed used for reproducibility. By setting a specific seed value,\n",
    "# you can ensure that the algorithm produces the same results each time it is run with the same data and parameters.\n",
    "\n",
    "# These parameters allow for customization and fine-tuning of the Isolation Forest algorithm based on the characteristics of the dataset\n",
    "# and the desired behavior of the anomaly detection process. Experimenting with different parameter settings can help optimize the algorithm's\n",
    "# performance for a specific application or dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eabb5eaa-6832-44b6-ad2b-61d2771825c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score \n",
    "# using KNN with K=10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f57f42b9-cd85-42e3-8f8d-96a688b3586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the anomaly score of a data point using k-nearest neighbors (KNN) with K=10, we need to consider the relative density of \n",
    "# the data point compared to its neighbors. In this case, if the data point has only 2 neighbors of the same class within a radius of 0.5,\n",
    "# the anomaly score can be computed as follows:\n",
    "\n",
    "# Calculate the distance to the Kth nearest neighbor:\n",
    "\n",
    "# Since K=10, we need to calculate the distance to the 10th nearest neighbor of the data point.\n",
    "# Determine the average distance of the K nearest neighbors:\n",
    "\n",
    "# Compute the average distance between the data point and its 10 nearest neighbors.\n",
    "# Normalize the average distance:\n",
    "\n",
    "# Normalize the average distance by dividing it by the distance to the Kth nearest neighbor.\n",
    "# Calculate the anomaly score:\n",
    "\n",
    "# The anomaly score can be defined as the inverse of the normalized average distance.\n",
    "# Anomaly Score = 1 / (Normalized Average Distance)\n",
    "\n",
    "# The specific values of the distances and the anomaly score would depend on the actual distances and the dataset.\n",
    "# However, based on the information provided, we can conclude that the data point has relatively few neighbors within a radius of 0.5 \n",
    "# and thus may have a higher anomaly score since it does not have many nearby instances of the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc8dc426-c0ee-41c2-8a8c-fc516593c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the \n",
    "# anomaly score for a data point that has an average path length of 5.0 compared to the average path \n",
    "# length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "750c8528-7de7-4514-b71e-d21d13b19587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The anomaly score in the Isolation Forest algorithm is calculated based on the average path length of a data point compared to the average path length of the trees.\n",
    "# In this case, if the data point has an average path length of 5.0 compared to the average path length of the trees, we can determine its anomaly score using\n",
    "# the following steps:\n",
    "\n",
    "# Compute the average path length of the trees:\n",
    "\n",
    "# The average path length of the trees is dependent on the number of data points and the number of trees in the Isolation Forest. \n",
    "# Since we have 100 trees and a dataset of 3000 data points, the average path length of the trees can be calculated as:\n",
    "# Average Path Length of Trees = 2 * (log2(3000) - 1) ≈ 14.9\n",
    "# Calculate the anomaly score:\n",
    "\n",
    "# The anomaly score is defined as the exponential function of the ratio between the average path length of the data point and the average path length of the trees.\n",
    "# Anomaly Score = 2^(-average path length ratio)\n",
    "# In this case, the average path length ratio is:\n",
    "# Average Path Length Ratio = Average Path Length of Data Point / Average Path Length of Trees\n",
    "# Average Path Length Ratio = 5.0 / 14.9 ≈ 0.3369\n",
    "\n",
    "# Thus, the anomaly score for the data point is:\n",
    "# Anomaly Score = 2^(-0.3369) ≈ 0.689\n",
    "\n",
    "# Please note that the anomaly score ranges from 0 to 1, where a higher score indicates a higher likelihood of the data point being an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e4550-7a32-41a7-aa29-e972a3c67bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
